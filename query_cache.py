import sqlite3
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import database
from db_schema import get_db_structure_hash
import logging

# Configura il logging per vedere gli errori nel container
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# âœ… Carichiamo un modello NLP leggero
model = SentenceTransformer("all-MiniLM-L6-v2")  # ğŸ”¥ Modello leggero ed efficiente


def encode_embedding(embedding):
    """Converte un vettore numpy in formato binario per salvarlo su SQLite."""
    return embedding.astype(np.float32).tobytes()


def decode_embedding(embedding_blob):
    """Converte un embedding binario in numpy array."""
    return np.frombuffer(embedding_blob, dtype=np.float32)



def get_cached_query(domanda):
    """
    Cerca una domanda simile nella cache usando FAISS per il confronto vettoriale.
    """
    domanda_vec = model.encode([domanda])[0]  # ğŸ”¹ Creiamo l'embedding della domanda
    current_db_hash = get_db_structure_hash()  # ğŸ”¹ Recuperiamo l'hash attuale del database

    conn = sqlite3.connect(database.DB_PATH)
    cursor = conn.cursor()
    # âœ… Selezioniamo solo le domande con lo stesso `db_hash`
    cursor.execute("""
        SELECT domanda, query_sql, embedding
        FROM query_cache
        WHERE db_hash = ?
    """, (current_db_hash,))
    cached_queries = cursor.fetchall()
    conn.close()

    if not cached_queries:
        return None  # ğŸ”¹ Nessuna query salvata, procediamo con OpenAI

    # ğŸ” Estraiamo gli embedding salvati
    embeddings = np.array([decode_embedding(row[2]) for row in cached_queries], dtype=np.float32)

    # âœ… Creiamo un indice FAISS per cercare la domanda piÃ¹ simile
    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 = distanza euclidea
    index.add(embeddings)

    # âœ… Troviamo la domanda piÃ¹ simile
    D, I = index.search(np.array([domanda_vec], dtype=np.float32), 1)

    best_match_idx = I[0][0]
    best_match_score = D[0][0]

    if best_match_score < 0.2:  # ğŸ”¥ Se Ã¨ abbastanza simile, la riutilizziamo

        logger.info(f"âœ… utilizzo di query on cache con somiglianza score {best_match_score}")
        return cached_queries[best_match_idx][1]  # âœ… Ritorniamo la query SQL salvata

    logger.info("âœ… nulla in cache generiamo una nuova query")
    return None  # ğŸ”¹ Se la somiglianza Ã¨ bassa, generiamo una nuova query


def save_query_to_cache(domanda, query_sql):
    """
    Salva una nuova domanda e query nella cache.
    """
    domanda_vec = model.encode([domanda])[0]  # ğŸ”¹ Creiamo l'embedding della domanda

    db_hash = get_db_structure_hash()  # ğŸ”¹ Recuperiamo l'hash attuale del database

    conn = sqlite3.connect(database.DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO query_cache (domanda, query_sql, db_hash, embedding) 
        VALUES (?, ?, ?, ?)
    """, (domanda, query_sql, db_hash, domanda_vec.astype(np.float32).tobytes()))
    conn.commit()
    conn.close()

    logger.info(f"âœ… inserita nuova entry in cache {domanda} {query_sql}")
